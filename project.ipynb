{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebd04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322e8737abbc4a10a25224d3c7ea3b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/912k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6574124ea824f60bc05e87fae077c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load and Clean Data from Hugging Face (Tatoeba)\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    return \" \".join(str(text).strip().lower().split())\n",
    "\n",
    "# Load Tatoeba from Hugging Face (filtered to Arabic-English)\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"ar\", lang2=\"en\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Convert to pandas and clean\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.dropna(subset=[\"translation\"])\n",
    "df['source'] = df['translation'].apply(lambda x: clean_text(x['ar']))\n",
    "df['target'] = df['translation'].apply(lambda x: clean_text(x['en']))\n",
    "df = df.drop(columns=[\"translation\"])\n",
    "\n",
    "# Use a subset for demo\n",
    "df = df.sample(2000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c288e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianTokenizer\n\u001b[32m      5\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mHelsinki-NLP/opus-mt-ar-en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[32m      8\u001b[39m dataset = HFDataset.from_pandas(df.rename(columns={\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mar\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m}))\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(example):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\OneDrive - Faculty of Computers and Information\\Level Three\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\OneDrive - Faculty of Computers and Information\\Level Three\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1826\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing for MarianMT\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = HFDataset.from_pandas(df.rename(columns={\"source\": \"ar\", \"target\": \"en\"}))\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"ar\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"en\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Pretrained Model\n",
    "from transformers import MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"marian-ar-en\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=True,\n",
    "    fp16=False,\n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Trainer Setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "# 6. Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2deefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Evaluate the Fine-Tuned Model\n",
    "from sacrebleu import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def translate_texts(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    translated = model.generate(**inputs)\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Sample test set\n",
    "test_samples = df.sample(100, random_state=1)\n",
    "sources = test_samples['source'].tolist()\n",
    "targets = test_samples['target'].tolist()\n",
    "\n",
    "# Fine-tuned predictions\n",
    "preds_finetuned = translate_texts(sources, model, tokenizer)\n",
    "bleu_finetuned = corpus_bleu(preds_finetuned, [targets])\n",
    "print(f\"[Fine-Tuned] BLEU Score: {bleu_finetuned.score:.2f}\")\n",
    "\n",
    "# Pretrained predictions\n",
    "model_pretrained = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n",
    "pretrained_preds = translate_texts(sources, model_pretrained, tokenizer)\n",
    "bleu_pretrained = corpus_bleu(pretrained_preds, [targets])\n",
    "print(f\"[Pretrained] BLEU Score: {bleu_pretrained.score:.2f}\")\n",
    "\n",
    "# Additional Metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "# Word-level comparison for accuracy and F1\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "true_words = flatten([t.split() for t in targets])\n",
    "pred_words = flatten([p.split() for p in preds_finetuned])\n",
    "\n",
    "# Ensure equal length\n",
    "min_len = min(len(true_words), len(pred_words))\n",
    "true_words, pred_words = true_words[:min_len], pred_words[:min_len]\n",
    "\n",
    "acc = accuracy_score(true_words, pred_words)\n",
    "prec, recall, f1, _ = precision_recall_fscore_support(true_words, pred_words, average='macro')\n",
    "\n",
    "print(f\"\\nWord-level Evaluation:\")\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "# BLEU Score Comparison Chart\n",
    "scores = [bleu_pretrained.score, bleu_finetuned.score]\n",
    "labels = [\"Pretrained\", \"Fine-Tuned\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=labels, y=scores, palette=\"Blues\")\n",
    "plt.title(\"BLEU Score Comparison\")\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()\n",
    "\n",
    "# Translate Sample\n",
    "example = \"أنا أحب البرمجة\"\n",
    "translated = translate_texts([example], model, tokenizer)[0]\n",
    "print(f\"\\nArabic: {example}\\nEnglish (Fine-tuned): {translated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdc622",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df751729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70705890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d46c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8246dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
