{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01misri\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ISRIStemmer\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01memoji\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Initial Setup with Detailed Logging\n",
    "def log_step(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"\\n[{timestamp}] {message}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "log_step(\"Starting Arabic-English Dataset Preprocessing Pipeline\")\n",
    "\n",
    "# 2. NLTK Resources Setup with Progress\n",
    "log_step(\"Setting up NLTK resources\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"✓ Successfully downloaded NLTK resources\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error downloading NLTK resources: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Load Stopwords with Verification\n",
    "log_step(\"Loading stopwords\")\n",
    "try:\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    print(f\"✓ Loaded {len(arabic_stopwords)} Arabic stopwords\")\n",
    "    print(f\"✓ Loaded {len(english_stopwords)} English stopwords\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading stopwords: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Dataset Loading with Detailed Stats\n",
    "log_step(\"Loading Tatoeba dataset\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    dataset = load_dataset(\"tatoeba\", lang1=\"ar\", lang2=\"en\", trust_remote_code=True)\n",
    "\n",
    "    if 'translation' in dataset['train'].features:\n",
    "        dataset = dataset.map(lambda x: {'ar': x['translation']['ar'], 'en': x['translation']['en']})\n",
    "\n",
    "    load_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"✓ Successfully loaded dataset in {load_time:.2f} seconds\")\n",
    "    print(f\"• Total samples: {len(dataset['train']):,}\")\n",
    "    print(f\"• First Arabic sample: {dataset['train'][0]['ar'][:50]}...\")\n",
    "    print(f\"• First English sample: {dataset['train'][0]['en'][:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Enhanced Text Cleaning\n",
    "def clean_text(text, lang, show_progress=False):\n",
    "    if show_progress:\n",
    "        print(f\"Processing {lang} text: {text[:30]}...\")\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    if lang == 'ar':\n",
    "        text = re.sub(r'[إأٱآا]', 'ا', text)\n",
    "        text = re.sub(r'ى', 'ي', text)\n",
    "        text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    stopwords_set = arabic_stopwords if lang == 'ar' else english_stopwords\n",
    "    words = [word for word in text.split() if word not in stopwords_set]\n",
    "    ## stem words of the same root \n",
    "    if lang == 'ar':\n",
    "        stemmer = ISRIStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    return text if len(text) > 1 else None\n",
    "\n",
    "\n",
    "log_step(\"Cleaning dataset\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    cleaned_dataset = dataset.map(lambda x: {\n",
    "        'ar': clean_text(x['ar'], 'ar'),\n",
    "        'en': clean_text(x['en'], 'en')\n",
    "    }).filter(lambda x: x['ar'] is not None and x['en'] is not None)\n",
    "\n",
    "    clean_time = (datetime.now() - start_time).total_seconds()\n",
    "    original_count = len(dataset['train'])\n",
    "    cleaned_count = len(cleaned_dataset['train'])\n",
    "    removed_count = original_count - cleaned_count\n",
    "\n",
    "    print(f\"✓ Cleaning completed in {clean_time:.2f} seconds\")\n",
    "    print(f\"• Original samples: {original_count:,}\")\n",
    "    print(f\"• Removed samples: {removed_count:,} ({removed_count / original_count:.2%})\")\n",
    "    print(f\"• Remaining samples: {cleaned_count:,}\")\n",
    "    print(\"\\nSample after cleaning:\")\n",
    "    print(f\"Arabic: {cleaned_dataset['train'][0]['ar']}\")\n",
    "    print(f\"English: {cleaned_dataset['train'][0]['en']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during cleaning: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Length Filtering\n",
    "log_step(\"Filtering by sentence length\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "\n",
    "\n",
    "    def filter_by_length(example):\n",
    "        ar_len = len(example['ar'].split())\n",
    "        en_len = len(example['en'].split())\n",
    "        return 3 <= ar_len <= 50 and 3 <= en_len <= 50\n",
    "\n",
    "\n",
    "    filtered_dataset = cleaned_dataset.filter(filter_by_length)\n",
    "    filter_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "    before_count = len(cleaned_dataset['train'])\n",
    "    after_count = len(filtered_dataset['train'])\n",
    "    removed = before_count - after_count\n",
    "\n",
    "    print(f\"✓ Filtering completed in {filter_time:.2f} seconds\")\n",
    "    print(f\"• Samples before filtering: {before_count:,}\")\n",
    "    print(f\"• Samples removed: {removed:,} ({removed / before_count:.2%})\")\n",
    "    print(f\"• Samples remaining: {after_count:,}\")\n",
    "\n",
    "    ar_lengths = [len(x['ar'].split()) for x in filtered_dataset['train']]\n",
    "    en_lengths = [len(x['en'].split()) for x in filtered_dataset['train']]\n",
    "\n",
    "    print(\"\\nLength distribution after filtering:\")\n",
    "    print(f\"• Arabic - Avg: {np.mean(ar_lengths):.1f}, Min: {min(ar_lengths)}, Max: {max(ar_lengths)}\")\n",
    "    print(f\"• English - Avg: {np.mean(en_lengths):.1f}, Min: {min(en_lengths)}, Max: {max(en_lengths)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during filtering: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Dataset Splitting\n",
    "log_step(\"Splitting dataset\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    df = pd.DataFrame(filtered_dataset['train'])\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    final_dataset = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_df),\n",
    "        'validation': Dataset.from_pandas(val_df),\n",
    "        'test': Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "    split_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"✓ Splitting completed in {split_time:.2f} seconds\")\n",
    "    print(f\"• Training set: {len(final_dataset['train']):,} samples\")\n",
    "    print(f\"• Validation set: {len(final_dataset['validation']):,} samples\")\n",
    "    print(f\"• Test set: {len(final_dataset['test']):,} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during splitting: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Tokenization\n",
    "log_step(\"Tokenizing data\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['ar'],\n",
    "            text_target=examples['en'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "\n",
    "\n",
    "    tokenized_dataset = final_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['ar', 'en']\n",
    "    )\n",
    "\n",
    "    tokenize_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"\\n✓ Tokenization completed in {tokenize_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during tokenization: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 9. Saving Data\n",
    "log_step(\"Saving processed data\")\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "    final_dataset.save_to_disk(\"processed_data/final_dataset\")\n",
    "    tokenized_dataset.save_to_disk(\"processed_data/tokenized_dataset\")\n",
    "\n",
    "    save_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"✓ Data saved in {save_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during saving: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Attention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from datasets import load_from_disk\n",
    "\n",
    "log_step(\"Starting Training Pipeline\")\n",
    "\n",
    "# 1. Load Tokenized Data\n",
    "try:\n",
    "    tokenized_dataset = load_from_disk(\"processed_data/tokenized_dataset\")\n",
    "    print(\"✓ Tokenized data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading tokenized data: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 2. Prepare Data for Training\n",
    "def prepare_data(dataset, max_length=64):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for example in dataset:\n",
    "        input_seq = example['input_ids'][:max_length]\n",
    "        label_seq = example['labels'][:max_length]\n",
    "\n",
    "        input_seq = np.pad(input_seq, (0, max_length - len(input_seq)),\n",
    "                           'constant', constant_values=tokenizer.pad_token_id)\n",
    "        label_seq = np.pad(label_seq, (0, max_length - len(label_seq)),\n",
    "                           'constant', constant_values=tokenizer.pad_token_id)\n",
    "\n",
    "        input_ids.append(input_seq)\n",
    "        labels.append(label_seq)\n",
    "\n",
    "    return np.array(input_ids), np.array(labels)\n",
    "\n",
    "\n",
    "log_step(\"Preparing training data\")\n",
    "try:\n",
    "    train_input_ids, train_labels = prepare_data(tokenized_dataset['train'])\n",
    "    val_input_ids, val_labels = prepare_data(tokenized_dataset['validation'])\n",
    "\n",
    "    train_decoder_input = np.roll(train_labels, 1, axis=1)\n",
    "    train_decoder_input[:, 0] = tokenizer.pad_token_id\n",
    "\n",
    "    val_decoder_input = np.roll(val_labels, 1, axis=1)\n",
    "    val_decoder_input[:, 0] = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"✓ Data prepared successfully\")\n",
    "    print(f\"• Training data shape: {train_input_ids.shape}\")\n",
    "    print(f\"• Validation data shape: {val_input_ids.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error preparing data: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Attention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from datasets import load_from_disk\n",
    "\n",
    "log_step(\"Starting Training Pipeline\")\n",
    "\n",
    "# Load Tokenized Data\n",
    "tokenized_dataset = load_from_disk(\"processed_data/tokenized_dataset\")\n",
    "\n",
    "# استخدم subset صغير للتجريب:\n",
    "sample_train = tokenized_dataset['train'].select(range(5000))\n",
    "sample_val = tokenized_dataset['validation'].select(range(1000))\n",
    "\n",
    "# إعداد البيانات\n",
    "def prepare_data(dataset, max_length=32):\n",
    "    input_ids, labels = [], []\n",
    "    for example in dataset:\n",
    "        input_seq = example['input_ids'][:max_length]\n",
    "        label_seq = example['labels'][:max_length]\n",
    "        input_seq = np.pad(input_seq, (0, max_length - len(input_seq)), 'constant', constant_values=tokenizer.pad_token_id)\n",
    "        label_seq = np.pad(label_seq, (0, max_length - len(label_seq)), 'constant', constant_values=tokenizer.pad_token_id)\n",
    "        input_ids.append(input_seq)\n",
    "        labels.append(label_seq)\n",
    "    return np.array(input_ids), np.array(labels)\n",
    "\n",
    "train_input_ids, train_labels = prepare_data(sample_train)\n",
    "val_input_ids, val_labels = prepare_data(sample_val)\n",
    "\n",
    "train_decoder_input = np.roll(train_labels, 1, axis=1)\n",
    "train_decoder_input[:, 0] = tokenizer.pad_token_id\n",
    "\n",
    "val_decoder_input = np.roll(val_labels, 1, axis=1)\n",
    "val_decoder_input[:, 0] = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# بناء الموديل بخيارات مخففة\n",
    "def build_translation_model(vocab_size, max_length=32):\n",
    "    encoder_inputs = Input(shape=(max_length,))\n",
    "    encoder_embedding = Embedding(vocab_size, 256)(encoder_inputs)\n",
    "    encoder_lstm = Bidirectional(LSTM(128, return_sequences=True))(encoder_embedding)\n",
    "\n",
    "    decoder_inputs = Input(shape=(max_length,))\n",
    "    decoder_embedding = Embedding(vocab_size, 256)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(128, return_sequences=True)(decoder_embedding)\n",
    "    decoder_dense_projection = Dense(256)(decoder_lstm)\n",
    "\n",
    "    attention = Attention()([encoder_lstm, decoder_dense_projection])\n",
    "    decoder_output = Dense(vocab_size, activation='softmax')(attention)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_translation_model(vocab_size=tokenizer.vocab_size)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# التدريب\n",
    "model.fit(\n",
    "    [train_input_ids, train_decoder_input],\n",
    "    np.expand_dims(train_labels, -1),\n",
    "    validation_data=([val_input_ids, val_decoder_input], np.expand_dims(val_labels, -1)),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_step(\"Previewing model outputs after training\")\n",
    "for i in range(5):\n",
    "    input_sample = train_input_ids[i]  # Get the input sequence (train_input_ids)\n",
    "    true_output = train_labels[i]  # Get the true label sequence (train_labels)\n",
    "\n",
    "    # Generate the model prediction for the input\n",
    "    model_output = model.predict([np.expand_dims(input_sample, axis=0), np.expand_dims(np.roll(true_output, 1, axis=1), axis=0)])\n",
    "    predicted_sequence = np.argmax(model_output, axis=-1).flatten()  # Decode the prediction (get the most probable tokens)\n",
    "\n",
    "    # Decode the predicted sequence and true labels back to tokens (you can use the tokenizer for this)\n",
    "    decoded_input = tokenizer.decode(input_sample, skip_special_tokens=True)\n",
    "    decoded_true_output = tokenizer.decode(true_output, skip_special_tokens=True)\n",
    "    decoded_predicted_output = tokenizer.decode(predicted_sequence, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Input (Arabic):\", decoded_input)\n",
    "    print(\"True Output (English):\", decoded_true_output)\n",
    "    print(\"Predicted Output (English):\", decoded_predicted_output)\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Training Visualization ===\n",
    "log_step(\"Plotting training history\")\n",
    "\n",
    "history = model.history.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation — Hugging Face vs Custom Keras Model\n",
    "### in this Evaluation We perform Translation on same data using the same tokenizer to Ensures fair comparison ( AutoTokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation Script — Hugging Face vs Custom Keras Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "tokenized_dataset = load_from_disk(\"processed_data/tokenized_dataset\")\n",
    "final_dataset = load_from_disk(\"processed_data/final_dataset\")\n",
    "test_texts = [ex['ar'] for ex in final_dataset['test'][:100]]\n",
    "test_refs = [ex['en'] for ex in final_dataset['test'][:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF model and tokenizer\n",
    "hf_model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "hf_model = AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n",
    "\n",
    "# Translate using Hugging Face\n",
    "def translate_hf(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    outputs = hf_model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "hf_preds = translate_hf(test_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom trained Keras model\n",
    "keras_model = tf.keras.models.load_model(\"best_model.keras\")\n",
    "\n",
    "# Prepare inputs for Keras\n",
    "def prepare_for_keras(texts, max_length=32):\n",
    "    input_ids = []\n",
    "    for text in texts:\n",
    "        ids = tokenizer(text, truncation=True, padding='max_length', max_length=max_length)['input_ids']\n",
    "        input_ids.append(ids)\n",
    "    return np.array(input_ids)\n",
    "\n",
    "keras_inputs = prepare_for_keras(test_texts)\n",
    "keras_decoder_input = np.zeros_like(keras_inputs)\n",
    "keras_decoder_input[:, 0] = tokenizer.pad_token_id  # Start decoding from padding\n",
    "\n",
    "keras_raw_preds = keras_model.predict([keras_inputs, keras_decoder_input])\n",
    "keras_token_ids = np.argmax(keras_raw_preds, axis=-1)\n",
    "keras_preds = [tokenizer.decode(ids, skip_special_tokens=True) for ids in keras_token_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score Comparison\n",
    "from sacrebleu import corpus_bleu\n",
    "hf_bleu = corpus_bleu(hf_preds, [test_refs])\n",
    "keras_bleu = corpus_bleu(keras_preds, [test_refs])\n",
    "\n",
    "# Word-level comparison\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "def word_level_metrics(preds, refs):\n",
    "    true_words = [w for ref in refs for w in ref.split()]\n",
    "    pred_words = [w for pred in preds for w in pred.split()]\n",
    "    min_len = min(len(true_words), len(pred_words))\n",
    "    true_words, pred_words = true_words[:min_len], pred_words[:min_len]\n",
    "    acc = accuracy_score(true_words, pred_words)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(true_words, pred_words, average='macro')\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "hf_acc, hf_prec, hf_rec, hf_f1 = word_level_metrics(hf_preds, test_refs)\n",
    "keras_acc, keras_prec, keras_rec, keras_f1 = word_level_metrics(keras_preds, test_refs)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n[BLEU Scores]\")\n",
    "print(f\"Hugging Face Model BLEU: {hf_bleu.score:.2f}\")\n",
    "print(f\"Keras Custom Model BLEU: {keras_bleu.score:.2f}\")\n",
    "\n",
    "print(\"\\n[Word-Level Metrics — Hugging Face]\")\n",
    "print(f\"Accuracy: {hf_acc:.2f}, Precision: {hf_prec:.2f}, Recall: {hf_rec:.2f}, F1: {hf_f1:.2f}\")\n",
    "\n",
    "print(\"\\n[Word-Level Metrics — Keras Model]\")\n",
    "print(f\"Accuracy: {keras_acc:.2f}, Precision: {keras_prec:.2f}, Recall: {keras_rec:.2f}, F1: {keras_f1:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=[\"HF BLEU\", \"Keras BLEU\"], y=[hf_bleu.score, keras_bleu.score], palette='Blues')\n",
    "plt.title(\"BLEU Score Comparison\")\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()\n",
    "\n",
    "# Print Sample Translations\n",
    "print(\"\\n[Translation Samples from Test Set]\")\n",
    "for i in range(5):\n",
    "    print(\"\\n--- Example\", i+1)\n",
    "    print(\"Arabic:\", test_texts[i])\n",
    "    print(\"Reference:\", test_refs[i])\n",
    "    print(\"HF Prediction:\", hf_preds[i])\n",
    "    print(\"Keras Prediction:\", keras_preds[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extran Manual Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Testing: Translate New Inputs\n",
    "print(\"\\n[Manual Test Examples]\")\n",
    "new_arabic_inputs = [\n",
    "    \"أين تقع القاهرة؟\",\n",
    "    \"أريد تعلم البرمجة\",\n",
    "    \"كيف حالك اليوم؟\"\n",
    "]\n",
    "\n",
    "new_hf_preds = translate_hf(new_arabic_inputs)\n",
    "new_keras_inputs = prepare_for_keras(new_arabic_inputs)\n",
    "new_keras_decoder_input = np.zeros_like(new_keras_inputs)\n",
    "new_keras_decoder_input[:, 0] = tokenizer.pad_token_id\n",
    "new_keras_raw_preds = keras_model.predict([new_keras_inputs, new_keras_decoder_input])\n",
    "new_keras_token_ids = np.argmax(new_keras_raw_preds, axis=-1)\n",
    "new_keras_preds = [tokenizer.decode(ids, skip_special_tokens=True) for ids in new_keras_token_ids]\n",
    "\n",
    "for ar, hf_pred, kr_pred in zip(new_arabic_inputs, new_hf_preds, new_keras_preds):\n",
    "    print(\"\\nInput:\", ar)\n",
    "    print(\"HF Translation:\", hf_pred)\n",
    "    print(\"Keras Translation:\", kr_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
